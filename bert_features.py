# -*- coding: utf-8 -*-
"""bert_features.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Gz9wnrHWOQPwI3Pn9-9dteAj9oFkXVZ7
"""

# !pip install transformers
# Extracting features from a BERT model
import torch
from transformers import BertTokenizer, BertModel
from scipy.spatial.distance import cosine
class BERTModelFeatures():
  def __init__(self):
    self.tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')
    self.model = BertModel.from_pretrained('bert-base-uncased', output_hidden_states = True).eval()
    self.features=None
  
  # Run the text through BERT, and collect all of the hidden states produced from all 12 layers. 
  def runBert(self,indexed_tokens):
    tokens_tensor = torch.tensor([indexed_tokens])
    with torch.no_grad():
        outputs = self.model(tokens_tensor)
        hidden_states = outputs[2]
        token_embeddings = torch.stack(hidden_states, dim=0)
        token_embeddings = torch.squeeze(token_embeddings, dim=1)
        token_embeddings = token_embeddings.permute(1,0,2)
    return hidden_states,token_embeddings
  
  def prapearingInput(self,sentence):
    marked_text = "[CLS] " + sentence + " [SEP]"
    tokenized_text = self.tokenizer.tokenize(marked_text)
    indexed_tokens = self.tokenizer.convert_tokens_to_ids(tokenized_text)
    return tokenized_text,indexed_tokens

  # creating the word vectors by summing together the last four layers.
  def tokenVecSum(self,sentence):
    tokenized_text,indexed_tokens=self.prapearingInput(sentence)
    hidden_states,token_embeddings=self.runBert(indexed_tokens)
    token_vecs_sum = []
    for token in token_embeddings:
        sum_vec = torch.sum(token[-4:], dim=0)
        token_vecs_sum.append(sum_vec)
    self.features={'hidden_states':hidden_states,
          'token_embeddings':token_embeddings,
          'tokenized_text':tokenized_text,
          'indexed_tokens':indexed_tokens,
          'token_vecs_sum':token_vecs_sum}
    return self.features
    
  # creating the word vectors by concatenate together the last four layers.
  def tokenVecCat(self,sentence):
    tokenized_text,indexed_tokens=self.prapearingInput(sentence)
    hidden_states,token_embeddings=self.runBert(indexed_tokens)
    token_vecs_cat = []
    for token in token_embeddings:
        cat_vec = torch.cat((token[-1], token[-2], token[-3], token[-4]), dim=0)
        token_vecs_cat.append(cat_vec)
    self.features={'hidden_states':hidden_states,
          'token_embeddings':token_embeddings,
          'tokenized_text':tokenized_text,
          'indexed_tokens':indexed_tokens,
          'token_vecs_cat':token_vecs_cat}
    return self.features
  
  # Calculate the cosine similarity between the vectors 
  def vecSimilarity(self,vec1,vec2):
    return (1 - cosine(vec1, vec2))
  
  # Returns a token vector for a specific token
  def token2Vec(self,tokenIdx=None,sum=True):
    tokenized_text=self.features['tokenized_text']
    for i, token_str in enumerate(tokenized_text):
        print (i, token_str)
    tokenIdx=int(input())
    print(self.features['token_vecs_sum'][tokenIdx])
    if sum:
      return self.features['token_vecs_sum'][tokenIdx]
    return self.features['token_vecs_cat'][tokenIdx]
    
#bF=BERTModelFeatures()
#bertFeatures=bF.tokenVecSum(text)
#v1=bF.token2Vec()
#v2=bF.token2Vec()
#bF.vecSimilarity(v1,v2)